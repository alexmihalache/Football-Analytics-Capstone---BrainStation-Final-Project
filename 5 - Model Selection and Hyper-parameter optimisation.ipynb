{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Plotting liibs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "#Football libs\n",
    "import socceraction\n",
    "from socceraction.data.statsbomb import StatsBombLoader\n",
    "from mplsoccer import Pitch, Sbopen, VerticalPitch\n",
    "import socceraction.spadl as spadl\n",
    "import matplotsoccer as mps\n",
    "import socceraction.xthreat as xthreat\n",
    "import socceraction.spadl as spadl\n",
    "from socceraction.vaep import VAEP\n",
    "\n",
    "# utils\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "# fuzz is used to compare TWO strings\n",
    "from fuzzywuzzy import fuzz\n",
    "# process is used to compare a string to MULTIPLE other strings\n",
    "from fuzzywuzzy import process\n",
    "import load_data\n",
    "import pre_processing_utils as ppu\n",
    "\n",
    "# ML libs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler,  MinMaxScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pre_processing_utils' from '/Users/alexmihalache/Library/CloudStorage/OneDrive-Personal/BrainStation/Capstone/Capstone Project - FAWSL Analysis/pre_processing_utils.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(ppu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "xt, xt_test, vaep, vaep_test, games, games_test, players, players_test, target_players = load_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_train_df = vaep.copy()\n",
    "modeling_test_df = vaep_test.copy()\n",
    "modeling_xt_train_df = xt.copy()\n",
    "modeling_xt_test_df = xt_test.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features, categorical_features, drop_features = col_transformer.set_ct_mode('team-vaep')\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "    # ('passthrough', passthrough_features),\n",
    "    ('drop', drop_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>player_name</th>\n",
       "      <th>minutes_played</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24239</td>\n",
       "      <td>Jemma Elizabeth Purfield</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15579</td>\n",
       "      <td>Inessa Kaagman</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5076</td>\n",
       "      <td>Emily Louise van Egmond</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5074</td>\n",
       "      <td>Shelina Laura Zadorsky</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31534</td>\n",
       "      <td>Ella Toone</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   player_id               player_name  minutes_played\n",
       "0      24239  Jemma Elizabeth Purfield            2016\n",
       "1      15579            Inessa Kaagman            2015\n",
       "2       5076   Emily Louise van Egmond            1940\n",
       "3       5074    Shelina Laura Zadorsky            1930\n",
       "4      31534                Ella Toone            1887"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>team_id</th>\n",
       "      <th>player_id</th>\n",
       "      <th>player_name</th>\n",
       "      <th>nickname</th>\n",
       "      <th>jersey_number</th>\n",
       "      <th>is_starter</th>\n",
       "      <th>starting_position_id</th>\n",
       "      <th>starting_position_name</th>\n",
       "      <th>minutes_played</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>19810</td>\n",
       "      <td>966</td>\n",
       "      <td>24239</td>\n",
       "      <td>Jemma Elizabeth Purfield</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>Left Back</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     game_id  team_id  player_id               player_name nickname  \\\n",
       "360    19810      966      24239  Jemma Elizabeth Purfield      NaN   \n",
       "\n",
       "     jersey_number  is_starter  starting_position_id starting_position_name  \\\n",
       "360             23        True                     6              Left Back   \n",
       "\n",
       "     minutes_played  \n",
       "360              96  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players[players['player_id']==24239].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>team_id</th>\n",
       "      <th>player_id</th>\n",
       "      <th>player_name</th>\n",
       "      <th>nickname</th>\n",
       "      <th>jersey_number</th>\n",
       "      <th>is_starter</th>\n",
       "      <th>starting_position_id</th>\n",
       "      <th>starting_position_name</th>\n",
       "      <th>minutes_played</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>3775636</td>\n",
       "      <td>973</td>\n",
       "      <td>24239</td>\n",
       "      <td>Jemma Elizabeth Purfield</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>Left Back</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     game_id  team_id  player_id               player_name nickname  \\\n",
       "371  3775636      973      24239  Jemma Elizabeth Purfield      NaN   \n",
       "\n",
       "     jersey_number  is_starter  starting_position_id starting_position_name  \\\n",
       "371             23        True                     6              Left Back   \n",
       "\n",
       "     minutes_played  \n",
       "371             103  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_test[players_test['player_id']==24239].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAEP Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_vaep = make_column_transformer(\n",
    "    (StandardScaler(), numeric_features_vaep),\n",
    "    (OneHotEncoder(handle_unknown='ignore'), categorical_features_vaep),\n",
    "    ('drop', drop_features_vaep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling \n",
    "\n",
    "**Approach to modeling:**\n",
    "\n",
    "1. Pick a player to replace in a team\n",
    "2. Get the top players in their cluster as potential replacements \n",
    "3. Fit a model for each of the scouted players \n",
    "    - use only that player's data to fit the model\n",
    "    - the models will predict their next location and the action they take \n",
    "    - the parameters in the model should cover the player characteristics and team characteristics. For example, the 5 past moves cover what the team does, and how then the player reacts to those. The player action is the predicted target \n",
    "\n",
    "**Model validation and testing approach:**\n",
    "\n",
    "- Of the scouted players, find those that have undergone a transfer in season 3 in our data set - use season 3 data to test our predictions. \n",
    "- Score the model based on the real data from their new team\n",
    "\n",
    "\n",
    "**Models to train and test:**\n",
    "\n",
    "- Baseline model for logistic regression for next action and end pitch location\n",
    "- Random Forest classifier for end pitch location and next action \n",
    "- xGBoost classifier for end pitch location and next action \n",
    "- Random Forest regressor for end_x and end_y location \n",
    "- xGBoost regressor for enx_x and end_y location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a class that:**\n",
    "\n",
    "1. Takes in a player id \n",
    "2. Slices the dataset for this player\n",
    "3. Clusters most similar players \n",
    "4. Take the top 3 players \n",
    "5. Slices the datasets for each player\n",
    "6. Fits a model for each \n",
    "7. Run a prediction based on the original player dataset\n",
    "8. Scores the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add in:\n",
    "\n",
    "kNN\n",
    "SVM\n",
    "DT\n",
    "Random Forest\n",
    "xGBoost\n",
    "\n",
    "with a cv 5\n",
    "\n",
    "Run it for End-zone \n",
    "Run it for Next Action\n",
    "Run it for VAEP Regression \n",
    "Run it for xT regression (build the dataset)\n",
    "\n",
    "add in kbest\n",
    "add in PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, y_train, X_test, y_test = ppu.create_player_data('regression', modeling_train_df, modeling_test_df, 15579, reg_target = 'vaep_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team - VAEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = ppu.create_team_data('team_id',965, modeling_train_df, modeling_test_df, 'vaep_value')\n",
    "\n",
    "# team_train_set = modeling_train_df[modeling_train_df['team_id']==965].dropna()\n",
    "# team_test_set = modeling_test_df[(modeling_test_df['team_id']==965)&(modeling_test_df['player_id']==15579)].dropna()\n",
    "# # team_test_set = modeling_test_df[modeling_test_df['team_id']==973].dropna()\n",
    "\n",
    "# X_train = team_train_set.drop(columns=['vaep_value'])\n",
    "# y_train_vaep = team_train_set['vaep_value']\n",
    "\n",
    "\n",
    "# X_test = team_test_set.drop(columns=['vaep_value'])\n",
    "# y_test_vaep = team_test_set['vaep_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features, categorical_features, drop_features = ppu.set_ct_mode('team-vaep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column transformer\n",
    "cat_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "num_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, numeric_features),\n",
    "    ('cat', cat_transformer, categorical_features),\n",
    "    ('drop', 'drop', drop_features)])\n",
    "\n",
    "estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('dim_reducer', PCA()),\n",
    "                       ('model', LinearRegression())], memory=cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 298 candidates, totalling 1490 fits\n",
      "[CV] END model=DecisionTreeRegressor(random_state=1), model__criterion=squared_error, model__max_depth=2, model__splitter=best; total time=   2.6s\n",
      "[CV] END model=DecisionTreeRegressor(random_state=1), model__criterion=squared_error, model__max_depth=2, model__splitter=best; total time=   2.5s\n",
      "[CV] END model=DecisionTreeRegressor(random_state=1), model__criterion=squared_error, model__max_depth=2, model__splitter=best; total time=   2.5s\n",
      "[CV] END model=DecisionTreeRegressor(random_state=1), model__criterion=squared_error, model__max_depth=2, model__splitter=best; total time=   2.5s\n",
      "[CV] END model=DecisionTreeRegressor(random_state=1), model__criterion=squared_error, model__max_depth=2, model__splitter=best; total time=   2.5s\n",
      "[CV] END model=DecisionTreeRegressor(random_state=1), model__criterion=squared_error, model__max_depth=2, model__splitter=random; total time=   1.7s\n",
      "[CV] END model=DecisionTreeRegressor(random_state=1), model__criterion=squared_error, model__max_depth=2, model__splitter=random; total time=   1.6s\n",
      "[CV] END model=DecisionTreeRegressor(random_state=1), model__criterion=squared_error, model__max_depth=2, model__splitter=random; total time=   1.7s\n",
      "[CV] END model=DecisionTreeRegressor(random_state=1), model__criterion=squared_error, model__max_depth=2, model__splitter=random; total time=   1.6s\n",
      "[CV] END model=DecisionTreeRegressor(random_state=1), model__criterion=squared_error, model__max_depth=2, model__splitter=random; total time=   1.7s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexmihalache/Library/CloudStorage/OneDrive-Personal/BrainStation/Capstone/Capstone Project - FAWSL Analysis/5 - Model Selection and Hyper-parameter optimisation.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexmihalache/Library/CloudStorage/OneDrive-Personal/BrainStation/Capstone/Capstone%20Project%20-%20FAWSL%20Analysis/5%20-%20Model%20Selection%20and%20Hyper-parameter%20optimisation.ipynb#X34sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Instantiate a gridsearch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexmihalache/Library/CloudStorage/OneDrive-Personal/BrainStation/Capstone/Capstone%20Project%20-%20FAWSL%20Analysis/5%20-%20Model%20Selection%20and%20Hyper-parameter%20optimisation.ipynb#X34sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(estimator, param_grid, cv \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexmihalache/Library/CloudStorage/OneDrive-Personal/BrainStation/Capstone/Capstone%20Project%20-%20FAWSL%20Analysis/5%20-%20Model%20Selection%20and%20Hyper-parameter%20optimisation.ipynb#X34sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m fitted_grid \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m         clone(base_estimator),\n\u001b[1;32m    841\u001b[0m         X,\n\u001b[1;32m    842\u001b[0m         y,\n\u001b[1;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    849\u001b[0m     )\n\u001b[1;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    852\u001b[0m     )\n\u001b[1;32m    853\u001b[0m )\n\u001b[1;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    679\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 680\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    682\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py:390\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \n\u001b[1;32m    366\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    389\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 390\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    391\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py:348\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    346\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    347\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    349\u001b[0m     cloned_transformer,\n\u001b[1;32m    350\u001b[0m     X,\n\u001b[1;32m    351\u001b[0m     y,\n\u001b[1;32m    352\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    353\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    354\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    355\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    357\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/memory.py:594\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cached_call(args, kwargs)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/memory.py:486\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cached_call\u001b[39m(\u001b[39mself\u001b[39m, args, kwargs, shelving\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    459\u001b[0m     \u001b[39m\"\"\"Call wrapped function and cache result, or read cache if available.\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \n\u001b[1;32m    461\u001b[0m \u001b[39m    This function returns the wrapped function output and some metadata.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39m        Some metadata about wrapped function call (see _persist_input()).\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m     func_id, args_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_output_identifiers(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    487\u001b[0m     metadata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/memory.py:638\u001b[0m, in \u001b[0;36mMemorizedFunc._get_output_identifiers\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39m\"\"\"Return the func identifier and input parameter hash of a result.\"\"\"\u001b[39;00m\n\u001b[1;32m    637\u001b[0m func_id \u001b[39m=\u001b[39m _build_func_identifier(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc)\n\u001b[0;32m--> 638\u001b[0m argument_hash \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_argument_hash(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    639\u001b[0m \u001b[39mreturn\u001b[39;00m func_id, argument_hash\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/memory.py:632\u001b[0m, in \u001b[0;36mMemorizedFunc._get_argument_hash\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_argument_hash\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 632\u001b[0m     \u001b[39mreturn\u001b[39;00m hashing\u001b[39m.\u001b[39;49mhash(filter_args(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore, args, kwargs),\n\u001b[1;32m    633\u001b[0m                         coerce_mmap\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmmap_mode \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:266\u001b[0m, in \u001b[0;36mhash\u001b[0;34m(obj, hash_name, coerce_mmap)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     hasher \u001b[39m=\u001b[39m Hasher(hash_name\u001b[39m=\u001b[39mhash_name)\n\u001b[0;32m--> 266\u001b[0m \u001b[39mreturn\u001b[39;00m hasher\u001b[39m.\u001b[39;49mhash(obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:63\u001b[0m, in \u001b[0;36mHasher.hash\u001b[0;34m(self, obj, return_digest)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhash\u001b[39m(\u001b[39mself\u001b[39m, obj, return_digest\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     62\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     64\u001b[0m     \u001b[39mexcept\u001b[39;00m pickle\u001b[39m.\u001b[39mPicklingError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m         e\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mPicklingError while hashing \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (obj, e),)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[0;32m--> 487\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(obj)\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[1;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:140\u001b[0m, in \u001b[0;36mHasher._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batch_setitems\u001b[39m(\u001b[39mself\u001b[39m, items):\n\u001b[1;32m    134\u001b[0m     \u001b[39m# forces order of keys in dict to ensure consistent hash.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         \u001b[39m# Trying first to compare dict assuming the type of keys is\u001b[39;00m\n\u001b[1;32m    137\u001b[0m         \u001b[39m# consistent and orderable.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         \u001b[39m# This fails on python 3 when keys are unorderable\u001b[39;00m\n\u001b[1;32m    139\u001b[0m         \u001b[39m# but we keep it in a try as it's faster.\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m         Pickler\u001b[39m.\u001b[39;49m_batch_setitems(\u001b[39mself\u001b[39;49m, \u001b[39miter\u001b[39;49m(\u001b[39msorted\u001b[39;49m(items)))\n\u001b[1;32m    141\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m         \u001b[39m# If keys are unorderable, sorting them using their hash. This is\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39m# slower but works in any case.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         Pickler\u001b[39m.\u001b[39m_batch_setitems(\u001b[39mself\u001b[39m, \u001b[39miter\u001b[39m(\u001b[39msorted\u001b[39m((\u001b[39mhash\u001b[39m(k), v)\n\u001b[1;32m    145\u001b[0m                                                   \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m items)))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    996\u001b[0m         save(k)\n\u001b[0;32m--> 997\u001b[0m         save(v)\n\u001b[1;32m    998\u001b[0m     write(SETITEMS)\n\u001b[1;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:971\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    970\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 971\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:140\u001b[0m, in \u001b[0;36mHasher._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batch_setitems\u001b[39m(\u001b[39mself\u001b[39m, items):\n\u001b[1;32m    134\u001b[0m     \u001b[39m# forces order of keys in dict to ensure consistent hash.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         \u001b[39m# Trying first to compare dict assuming the type of keys is\u001b[39;00m\n\u001b[1;32m    137\u001b[0m         \u001b[39m# consistent and orderable.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         \u001b[39m# This fails on python 3 when keys are unorderable\u001b[39;00m\n\u001b[1;32m    139\u001b[0m         \u001b[39m# but we keep it in a try as it's faster.\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m         Pickler\u001b[39m.\u001b[39;49m_batch_setitems(\u001b[39mself\u001b[39;49m, \u001b[39miter\u001b[39;49m(\u001b[39msorted\u001b[39;49m(items)))\n\u001b[1;32m    141\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m         \u001b[39m# If keys are unorderable, sorting them using their hash. This is\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39m# slower but works in any case.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         Pickler\u001b[39m.\u001b[39m_batch_setitems(\u001b[39mself\u001b[39m, \u001b[39miter\u001b[39m(\u001b[39msorted\u001b[39m((\u001b[39mhash\u001b[39m(k), v)\n\u001b[1;32m    145\u001b[0m                                                   \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m items)))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:997\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    996\u001b[0m         save(k)\n\u001b[0;32m--> 997\u001b[0m         save(v)\n\u001b[1;32m    998\u001b[0m     write(SETITEMS)\n\u001b[1;32m    999\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:692\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     save(func)\n\u001b[0;32m--> 692\u001b[0m     save(args)\n\u001b[1;32m    693\u001b[0m     write(REDUCE)\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     \u001b[39m# If the object is already in the memo, this means it is\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[39m# recursive. In this case, throw away everything we put on the\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[39m# stack, and fetch the object back from the memo.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:886\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    885\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 886\u001b[0m         save(element)\n\u001b[1;32m    887\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[1;32m    888\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:901\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    899\u001b[0m write(MARK)\n\u001b[1;32m    900\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 901\u001b[0m     save(element)\n\u001b[1;32m    903\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n\u001b[1;32m    904\u001b[0m     \u001b[39m# Subtle.  d was not in memo when we entered save_tuple(), so\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     \u001b[39m# the process of saving the tuple's elements must have saved\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[39m# could have been done in the \"for element\" loop instead, but\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[39m# recursive tuples are a rare thing.\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     get \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(memo[\u001b[39mid\u001b[39m(obj)][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:692\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     save(func)\n\u001b[0;32m--> 692\u001b[0m     save(args)\n\u001b[1;32m    693\u001b[0m     write(REDUCE)\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     \u001b[39m# If the object is already in the memo, this means it is\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[39m# recursive. In this case, throw away everything we put on the\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[39m# stack, and fetch the object back from the memo.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:886\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    885\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 886\u001b[0m         save(element)\n\u001b[1;32m    887\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[1;32m    888\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:901\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    899\u001b[0m write(MARK)\n\u001b[1;32m    900\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 901\u001b[0m     save(element)\n\u001b[1;32m    903\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n\u001b[1;32m    904\u001b[0m     \u001b[39m# Subtle.  d was not in memo when we entered save_tuple(), so\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     \u001b[39m# the process of saving the tuple's elements must have saved\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[39m# could have been done in the \"for element\" loop instead, but\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[39m# recursive tuples are a rare thing.\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     get \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(memo[\u001b[39mid\u001b[39m(obj)][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:931\u001b[0m, in \u001b[0;36m_Pickler.save_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m LIST)\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 931\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_appends(obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:955\u001b[0m, in \u001b[0;36m_Pickler._batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    953\u001b[0m     write(MARK)\n\u001b[1;32m    954\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tmp:\n\u001b[0;32m--> 955\u001b[0m         save(x)\n\u001b[1;32m    956\u001b[0m     write(APPENDS)\n\u001b[1;32m    957\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/pickle.py:870\u001b[0m, in \u001b[0;36m_Pickler.save_str\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    867\u001b[0m     obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\x1a\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mu001a\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# EOF on DOS\u001b[39;00m\n\u001b[1;32m    868\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(UNICODE \u001b[39m+\u001b[39m obj\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mraw-unicode-escape\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m\n\u001b[1;32m    869\u001b[0m                \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 870\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemoize(obj)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define a parameter grid\n",
    "param_grid = [\n",
    "     {\n",
    "        'model': [RandomForestRegressor(random_state=1, oob_score=True, n_jobs=-1)],\n",
    "        'model__max_depth': list(range(2, 25, 2)),\n",
    "        'model__max_features': ['sqrt', 'auto', 'log2'],\n",
    "        'model__n_estimators': list(range(10, 100, 10))}, \n",
    "    \n",
    "     {\n",
    "        'model': [xgb.XGBRegressor(random_state=1)],\n",
    "        'model__max_depth': list(range(2, 11, 1))},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate a gridsearch\n",
    "grid = GridSearchCV(estimator, param_grid, cv = 5, verbose = 2)\n",
    "fitted_grid = grid.fit(X_train, y_train)\n",
    "\n",
    "# fitted_grid.best_estimator_\n",
    "# fitted_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory='/var/folders/t5/kc4tlr5n2yn5jkkh1t9rk9500000gn/T/tmpzw0z4314',\n",
      "         steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['start_x', 'start_y',\n",
      "                                                   'end_x', 'end_y', 'x_dif',\n",
      "                                                   'y_dif', 'time_seconds',\n",
      "                                                   'n-1_x_distance',\n",
      "                                                   'n-1_y_distance',\n",
      "                                                   'n-1_start_x', 'n-1_start_y',\n",
      "                                                   'n-1_end_x', 'n-1_end_y',\n",
      "                                                   'n-2_x_di...\n",
      "                                                   'n-5_defensive_value',\n",
      "                                                   'n-5_vaep_value',\n",
      "                                                   'n-4_offensive_value',\n",
      "                                                   'n-4_defensive_value',\n",
      "                                                   'n-4_vaep_value',\n",
      "                                                   'n-3_offensive_value',\n",
      "                                                   'n-3_defensive_value',\n",
      "                                                   'n-3_vaep_value',\n",
      "                                                   'n-2_offensive_value',\n",
      "                                                   'n-2_defensive_value',\n",
      "                                                   'n-2_vaep_value',\n",
      "                                                   'n-1_offensive_value',\n",
      "                                                   'n-1_defensive_value', ...])])),\n",
      "                ('dim_reducer', PCA(n_components=0.95)),\n",
      "                ('model', Ridge(alpha=9, random_state=1))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06629592197157597"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fitted_grid.best_estimator_)\n",
    "fitted_grid.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b230f2c3a454b9d31000333b2514d91645996906422c6e0b66023d65cd0ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
